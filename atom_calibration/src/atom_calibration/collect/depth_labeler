#!/usr/bin/env python3


# stdlib
import argparse
import math
import os
import threading


# 3rd-party
import colorama
from colorama import Fore, Style
import cv2
import rospy
import numpy as np
import sensor_msgs.point_cloud2 as pc2
import image_geometry
import atom_core.utilities
from cv_bridge import CvBridge
from matplotlib import cm
from sensor_msgs import point_cloud2
from std_msgs.msg import Header
from visualization_msgs.msg import Marker, InteractiveMarker, InteractiveMarkerControl
from rospy_message_converter import message_converter
from sensor_msgs.msg import *
from sensor_msgs.msg import PointField, CameraInfo, Image
from interactive_markers.interactive_marker_server import InteractiveMarkerServer
from geometry_msgs.msg import PointStamped

# local packages
from atom_core.config_io import loadConfig
from atom_core.utilities import atomError
import atom_core.ros_utils

from atom_calibration.collect import patterns
from atom_calibration.collect.label_messages import labelPointCloud2Msg, labelDepthMsg, numpyFromPointCloudMsg


# The data structure of each point in ros PointCloud2: 16 bits = x + y + z + rgb
FIELDS_XYZ = [
    PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
    PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
    PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
]
FIELDS_XYZRGB = FIELDS_XYZ + \
    [PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)]

# Bit operations
BIT_MOVE_16 = 2 ** 16
BIT_MOVE_8 = 2 ** 8


def createRosCloud(points, stamp, frame_id, colours=None):
    # Set header
    header = Header()
    header.stamp = stamp
    header.frame_id = frame_id

    # Set "fields" and "cloud_data"
    points = np.asarray(points)
    if not colours:  # XYZ only
        fields = FIELDS_XYZ
        cloud_data = points
    else:  # XYZ + RGB
        fields = FIELDS_XYZRGB
        tmpColours = np.floor(np.asarray(colours) * 255)  # nx3 matrix
        tmpColours = tmpColours[:, 0] * BIT_MOVE_16 + tmpColours[:, 1] * BIT_MOVE_8 + tmpColours[:, 2]
        cloud_data = np.c_[points, tmpColours]

    # Create ros_cloud
    return pc2.create_cloud(header, fields, cloud_data)


# ------------------------
#      BASE CLASSES      #
# ------------------------


class LaserScanCluster:
    """
    An auxiliary class for storing information about 2D laser clusters
    """

    def __init__(self, cluster_count, idx):
        self.cluster_count = cluster_count
        self.idxs = [idx]

    def pushIdx(self, idx):
        self.idxs.append(idx)

    def __str__(self):
        return "Cluster " + str(self.cluster_count) + " contains idxs = " + str(self.idxs)


class DepthLabeler:
    """ Semi-automated labeling. An rviz interactive marker is placed inside the camera frustum overlapping the
        calibration pattern and the pattern is tracked from that seed point using a propagation mask. The
        pattern is tracked automatically from there onward by assuming that the centroid of the calibration
        pattern's shape is the seed point in  the next frame.
    """

    def __init__(self, config, sensor_name, label_data=True, debug=False, marker_scale=0.5, color=(255, 0, 0)):
        # def __init__(self, server, menu_handler, sensor_dict, marker_scale, calib_pattern, color, label_data=True):

        print('Creating a depth labeler for sensor ' + sensor_name)

        # Store variables to class attributes
        self.label_data = label_data
        self.config = config
        self.sensor_name = sensor_name
        self.marker_scale = marker_scale
        self.color = color
        self.debug = debug

        # Check if sensor exists in config
        if self.sensor_name not in config['sensors'].keys():
            atomError('Sensor ' + Fore.BLUE + sensor_name + Style.RESET_ALL + ' not in config. Cannot start labeler.\nAvailable sensors are: ' +
                      Fore.BLUE + str(list(config['sensors'].keys())) + Style.RESET_ALL)

        self.sensor_config = config['sensors'][sensor_name]

        # Check if modality is lidar3d
        if not self.sensor_config['modality'] == 'depth':
            atomError('Sensor ' + sensor_name + ' has modality ' +
                      self.sensor_config['modality'] + ' . Cannot start depth labeler.')

        # Get the type of message from the message topic of the sensor data, which is given as input. The message
        self.msg_type_str, self.msg_type = atom_core.ros_utils.getMessageTypeFromTopic(self.sensor_config['topic_name'])

        # TODO for now this will only work with a single pattern. Must extend this to multi-pattern detection later
        if len(self.config['calibration_patterns'].keys()) > 1:
            atomError('Depth labeler is not prepared to detect multiple patterns.')

        self.pattern_key = list(self.config['calibration_patterns'].keys())[0]
        self.pattern = self.config['calibration_patterns'][self.pattern_key]

        # Set up the publication of the labeled image
        self.bridge = CvBridge()  # a CvBridge structure is needed to convert opencv images to ros messages.
        self.publisher_labelled_depth = rospy.Publisher(self.sensor_config['topic_name'] + '/labeled',
                                                        sensor_msgs.msg.Image,
                                                        queue_size=1)

        # Receive one camera_info message corresponding to this sensor and store it.
        # We need this to have the K and D matrices.

        print('topic name is ' + self.sensor_config['topic_name'])

        print('os.path is ' + self.sensor_config['topic_name'])

        camera_info_topic = os.path.dirname(self.sensor_config['topic_name']) + '/camera_info'
        print('Waiting for camera_info message on topic ' + camera_info_topic + ' ...')
        self.camera_info_msg = rospy.wait_for_message(camera_info_topic, CameraInfo)
        print(' received!')

        # Use image resolution to define initial seed in the middle of the image
        self.width = self.camera_info_msg.width
        self.height = self.camera_info_msg.height
        self.seed = {'x': round(self.width / 2), 'y': round(self.height / 2)}
        self.pyrdown = 1

        self.subscriber_mouse_click = rospy.Subscriber(self.sensor_config['topic_name'] + '/labeled/mouse_click',
                                                       PointStamped,
                                                       self.mouseClickReceivedCallback)

        # Subscribe to the message topic containing sensor data
        self.subscriber = rospy.Subscriber(self.sensor_config['topic_name'],
                                           self.msg_type,
                                           self.labelData,
                                           queue_size=1)

        self.tic_manual_seed = rospy.Time.now()

    def mouseClickReceivedCallback(self, msg):
        self.seed['x'] = msg.point.x * pow(2, self.pyrdown)
        self.seed['y'] = msg.point.y * pow(2, self.pyrdown)
        self.tic_manual_seed = rospy.Time.now()
        if self.debug:
            print('Setting new seed point for sensor ' + self.sensor_name + ' to ' + str(self.seed))

    def labelData(self, msg):

        if self.debug:
            print('labeling data for sensor ' + self.sensor_name)

        # Reset detected and idxs values to make sure we are not using information from a previous labeling

        filter_border_edges = 0.025

        # The actual labeling procedure
        labels, result_image, new_seed_point = labelDepthMsg(msg, seed=self.seed,
                                                             bridge=self.bridge, pyrdown=self.pyrdown,
                                                             scatter_seed=True, debug=False,
                                                             subsample_solid_points=3, limit_sample_step=1, filter_border_edges=filter_border_edges)

        # print('new_seed_point = ' + str(new_seed_point))
        time_since_manual_seed = (rospy.Time.now()-self.tic_manual_seed).to_sec()

        # https://github.com/lardemua/atom/issues/639
        if time_since_manual_seed > 1.0:  # only track after 1 second of setting manual seed.
            if 0 < new_seed_point['x'] < self.width-filter_border_edges*self.width and \
               0 < new_seed_point['y'] < self.height-filter_border_edges*self.height:
                self.seed['x'] = new_seed_point['x']
                self.seed['y'] = new_seed_point['y']
            else:
                self.seed = {'x': round(self.width / 2), 'y': round(self.height / 2)}

        msg_out = self.bridge.cv2_to_imgmsg(result_image, encoding="passthrough")
        msg_out.header.stamp = msg.header.stamp
        msg_out.header.frame_id = msg.header.frame_id
        self.publisher_labelled_depth.publish(msg_out)


def main():

    # Parse command line arguments
    ap = argparse.ArgumentParser()
    ap.add_argument("-cfg", "--config", help='Calibration config file.', type=str, required=True)
    ap.add_argument("-sn", "--sensor_name", help='Name of the sensor as given in the config.yml', type=str, required=True)
    ap.add_argument("-d", "--debug", help='Run in debug mode', action='store_true', default=False)
    ap.add_argument("-c", "--color", nargs=3, help='Color associated with this labeler', default=[255, 0, 0], type=int)

    args = vars(ap.parse_args(args=atom_core.ros_utils.filterLaunchArguments(sys.argv)))

    config = loadConfig(args['config'])

    # Initialize ROS stuff
    node_name = args['sensor_name'] + '_lidar3d_labeler'
    rospy.init_node(node_name)

    lidar_labeler = DepthLabeler(config, sensor_name=args['sensor_name'], debug=args['debug'], color=args['color'])
    rospy.spin()


if __name__ == '__main__':
    main()
